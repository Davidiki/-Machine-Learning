{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos Baseados em Árvores**\n",
        "Os modelos baseados em árvores são uma classe poderosa de algoritmos de aprendizado de máquina utilizados tanto para tarefas de classificação quanto de regressão. Eles funcionam construindo uma estrutura hierárquica (árvore) de decisões baseadas nas features dos dados, permitindo capturar relações não lineares e interações complexas entre as variáveis.\n",
        "\n",
        "**Características dos Modelos Baseados em Árvores**\n",
        "- **Flexibilidade:** Capazes de modelar relações não lineares e interações entre features.\n",
        "- **Interpretabilidade:** Árvores de decisão são intuitivas e fáceis de visualizar.\n",
        "- **Robustez:** Modelos como Random Forest e Gradient Boosting são robustos a overfitting quando bem ajustados.\n",
        "- **Escalabilidade:** Podem ser eficientes em conjuntos de dados de grande dimensão com as devidas otimizações.\n",
        "\n",
        "No entanto, os modelos baseados em árvores também apresentam limitações, como a tendência a `overfitting` (especialmente árvores individuais profundas), e a necessidade de ajuste cuidadoso dos `hiperparâmetros` para alcançar um desempenho ótimo."
      ],
      "metadata": {
        "id": "4NqNHvGctds5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas necessárias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Importando modelos baseados em árvores do scikit-learn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "\n",
        "# Importando XGBoost e LightGBM\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except ImportError:\n",
        "    !pip install xgboost\n",
        "    import xgboost as xgb\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except ImportError:\n",
        "    !pip install lightgbm\n",
        "    import lightgbm as lgb\n",
        "\n",
        "# Carregando o dataset breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Dividindo o dataset em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Função para treinar, prever e avaliar modelos\n",
        "def train_evaluate_model(model, model_name, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"### {model_name}\")\n",
        "    print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8gTA678whWG",
        "outputId": "36643e92-eb60-49ee-f122-1bf7926edaca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Decision Tree (Árvore de Decisão)**\n",
        "**Funcionamento**\n",
        "A Árvore de Decisão é o modelo base para muitos algoritmos de árvore. Ela divide recursivamente o espaço de features em regiões mais homogêneas em termos da variável alvo. Em cada nó interno, uma feature e um limiar são selecionados para melhor separar as classes (no caso de classificação) ou minimizar o erro (no caso de regressão).\n",
        "\n",
        "**Pontos Fortes**\n",
        "- Interpretabilidade: Fácil de visualizar e entender as decisões tomadas.\n",
        "- Não Requer Pré-processamento: Não necessita de normalização ou padronização das features.\n",
        "- Capacidade de Capturar Relações Não Lineares: Pode modelar interações complexas entre as features.\n",
        "\n",
        "**Pontos Fracos**\n",
        "- Tendência a Overfitting: Árvores profundas podem se ajustar excessivamente aos dados de treinamento.\n",
        "- Instabilidade: Pequenas variações nos dados podem resultar em árvores muito diferentes.\n",
        "- Baixo Desempenho em Dados Complexos: Árvores únicas podem não capturar todas as nuances dos dados."
      ],
      "metadata": {
        "id": "S7_YilCmuFI1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHCeVvpGtbOO",
        "outputId": "3c37e3d6-1c1c-4583-a03c-8760110bf204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Decision Tree\n",
            "Acurácia: 0.9415204678362573\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92        63\n",
            "           1       0.97      0.94      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.93      0.94      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Decision Tree\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "train_evaluate_model(dt_clf, \"Decision Tree\", X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**\n",
        "**Funcionamento:**\n",
        "O Random Forest é um ensemble de Árvores de Decisão. Ele constrói múltiplas árvores durante o treinamento e combina suas previsões por votação (classificação) ou média (regressão). Cada árvore é treinada em um subconjunto aleatório dos dados e das features, promovendo a diversidade entre as árvores.\n",
        "\n",
        "**Pontos Fortes**\n",
        "- Redução de Overfitting: A média de múltiplas árvores reduz a variância e melhora a generalização.\n",
        "- Importância das Features: Fornece métricas de importância das features.\n",
        "- Robustez: Funciona bem com dados ruidosos e de alta dimensionalidade.\n",
        "\n",
        "**Pontos Fracos**\n",
        "- Menos Interpretável: Mais difícil de visualizar e interpretar comparado a uma única árvore.\n",
        "- Maior Custo Computacional: Requer mais memória e tempo de processamento devido à construção de múltiplas árvores.\n",
        "- Não Funciona Bem com Dados de Sequência: Pode ter desempenho limitado em dados com dependências temporais."
      ],
      "metadata": {
        "id": "YvVZDSdAuaVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Random Forest\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "train_evaluate_model(rf_clf, \"Random Forest\", X_train, X_test, y_train, y_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z2GaPABun-5",
        "outputId": "437e41ef-6046-4c9d-f5fe-60cd1ab44601"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Random Forest\n",
            "Acurácia: 0.9707602339181286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96        63\n",
            "           1       0.96      0.99      0.98       108\n",
            "\n",
            "    accuracy                           0.97       171\n",
            "   macro avg       0.97      0.96      0.97       171\n",
            "weighted avg       0.97      0.97      0.97       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Extra Trees (Extremely Randomized Trees)**\n",
        "**Funcionamento:**\n",
        "O Extra Trees é semelhante ao Random Forest, mas com uma maior aleatoriedade na construção das árvores. Ele seleciona os cortes das features de forma completamente aleatória, ao invés de buscar o melhor corte possível. Isso pode levar a uma redução do overfitting e a uma melhoria na velocidade de treinamento.\n",
        "\n",
        "**Pontos Fortes**\n",
        "- Rapidez: Treinamento mais rápido comparado ao Random Forest devido à aleatoriedade adicional.\n",
        "- Redução de Overfitting: Aumenta a diversidade das árvores, melhorando a generalização.\n",
        "- Bom Desempenho em Geral: Frequentemente apresenta desempenho similar ou superior ao Random Forest.\n",
        "\n",
        "**Pontos Fracos**\n",
        "- Menos Precisão em Algumas Situações: A aleatoriedade extra pode, em alguns casos, levar a uma ligeira queda na precisão.\n",
        "- Interpretabilidade Reduzida: Semelhante ao Random Forest, é difícil de interpretar individualmente.\n",
        "- Dependência de Hiperparâmetros: Requer ajuste cuidadoso dos hiperparâmetros para maximizar o desempenho."
      ],
      "metadata": {
        "id": "yWgYsvkpuoOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Extra Trees\n",
        "et_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "train_evaluate_model(et_clf, \"Extra Trees\", X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43aSB-Olu3ZM",
        "outputId": "ef814d1f-83a3-482f-b04b-c06b2520c2f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Extra Trees\n",
            "Acurácia: 0.9766081871345029\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97        63\n",
            "           1       0.97      0.99      0.98       108\n",
            "\n",
            "    accuracy                           0.98       171\n",
            "   macro avg       0.98      0.97      0.97       171\n",
            "weighted avg       0.98      0.98      0.98       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Gradient Boosting (Boosting por Gradiente)**\n",
        "**Funcionamento:**\n",
        "O Gradient Boosting constrói um modelo preditivo de forma sequencial, onde cada novo modelo tenta corrigir os erros dos modelos anteriores. Utiliza o gradiente da função de perda para direcionar a construção das novas árvores, otimizando a performance do conjunto.\n",
        "\n",
        "**Pontos Fortes**\n",
        "- Alto Desempenho: Geralmente alcança resultados de ponta em várias competições de machine learning.\n",
        "- Flexibilidade: Pode otimizar diferentes funções de perda e ser adaptado para várias tarefas.\n",
        "- Importância das Features: Fornece métricas de importância das features.\n",
        "\n",
        "**Pontos Fracos**\n",
        "- Treinamento Lento: Processo sequencial pode ser computacionalmente intensivo.\n",
        "- Sensível a Outliers: Pode ajustar excessivamente os outliers nos dados.\n",
        "- Requer Ajuste Cuidadoso: Muitos hiperparâmetros a serem ajustados para evitar overfitting."
      ],
      "metadata": {
        "id": "Qyy3cxOqu3mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Gradient Boosting\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "train_evaluate_model(gb_clf, \"Gradient Boosting\", X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIVJg6PqvGnJ",
        "outputId": "a323f1ec-4a3c-474b-ea99-1a718e16472e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Gradient Boosting\n",
            "Acurácia: 0.9590643274853801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94        63\n",
            "           1       0.96      0.97      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.96      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**AdaBoost (Adaptive Boosting)**\n",
        "**Funcionamento:**\n",
        "O AdaBoost é um algoritmo de boosting que combina múltiplos classificadores fracos (geralmente árvores de decisão rasas) em um classificador forte. Ele ajusta os pesos das amostras após cada iteração, dando mais importância às amostras que foram classificadas incorretamente.\n",
        "\n",
        "**Pontos Fortes**\n",
        "- Simplicidade: Fácil de implementar e entender.\n",
        "- Redução de Erro: Pode melhorar significativamente o desempenho de classificadores fracos.\n",
        "- Robustez a Overfitting: Menos propenso a overfitting em comparação com alguns outros métodos de boosting.\n",
        "\n",
        "**Pontos Fracos**\n",
        "- Sensível a Outliers: Outliers podem receber altos pesos, afetando negativamente o desempenho.\n",
        "- Dependência dos Classificadores Fracos: Desempenho limitado pela qualidade dos classificadores base.\n",
        "- Menor Precisão em Comparação com Boosting Avançado: Pode não alcançar a mesma precisão que métodos como Gradient Boosting ou XGBoost."
      ],
      "metadata": {
        "id": "ssXNCSUkvG9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. AdaBoost\n",
        "ada_clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "train_evaluate_model(ada_clf, \"AdaBoost\", X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcoJDhIevurj",
        "outputId": "315312d9-f441-48c5-e8f6-f260f3a80082"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### AdaBoost\n",
            "Acurácia: 0.9532163742690059\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94        63\n",
            "           1       0.96      0.96      0.96       108\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.95      0.95      0.95       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Bagging (Bootstrap Aggregating)**\n",
        "**Funcionamento:**\n",
        "O Bagging é uma técnica de ensemble que constrói múltiplos modelos treinados em diferentes subconjuntos do conjunto de dados original (obtidos por amostragem com reposição). As previsões dos modelos são então agregadas por votação ou média para formar a previsão final.\n",
        "\n",
        "**Pontos Fortes**\n",
        "- Redução de Overfitting: A média ou votação de múltiplos modelos reduz a variância.\n",
        "- Versatilidade: Pode ser aplicado a vários tipos de modelos base.\n",
        "- Melhoria de Desempenho: Geralmente melhora a precisão e a robustez dos modelos individuais.\n",
        "\n",
        "**Pontos Fracos**\n",
        "- Maior Custo Computacional: Requer treinamento de múltiplos modelos, aumentando o tempo de processamento.\n",
        "- Menos Interpretável: Difícil de interpretar o modelo final como um todo.\n",
        "- Benefícios Limitados para Alguns Modelos: Pode não melhorar significativamente modelos que já são robustos."
      ],
      "metadata": {
        "id": "djwg949lvu8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Bagging\n",
        "bag_clf = BaggingClassifier(n_estimators=100, random_state=42)\n",
        "train_evaluate_model(bag_clf, \"Bagging\", X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "682fCQfHv-NX",
        "outputId": "744b9a29-b508-4091-be5a-52eb968c8532"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Bagging\n",
            "Acurácia: 0.9590643274853801\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94        63\n",
            "           1       0.96      0.97      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.96      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**XGBoost (Extreme Gradient Boosting)**\n",
        "**Funcionamento**\n",
        "O XGBoost é uma implementação otimizada de Gradient Boosting que utiliza técnicas avançadas como regularização, paralelização e processamento distribuído. É projetado para ser altamente eficiente, flexível e portátil, oferecendo alto desempenho em diversas tarefas de machine learning.\n",
        "\n",
        "**Pontos Fortes**\n",
        "- Alto Desempenho: Frequentemente vence competições de machine learning devido à sua eficiência e capacidade de otimização.\n",
        "- Regularização: Inclui termos de regularização que ajudam a prevenir overfitting.\n",
        "- Flexibilidade: Suporta diferentes tipos de funções de perda e personalizações.\n",
        "\n",
        "**Pontos Fracos**\n",
        "- Complexidade: Mais complexo de configurar e otimizar comparado a outros algoritmos.\n",
        "- Requer Conhecimento Avançado: Para tirar o máximo proveito, é necessário entender profundamente os hiperparâmetros e o funcionamento interno.\n",
        "- Consumo de Memória: Pode ser intensivo em memória para grandes conjuntos de dados."
      ],
      "metadata": {
        "id": "IIpOxgcZv-de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. XGBoost\n",
        "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "train_evaluate_model(xgb_clf, \"XGBoost\", X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJp_JP2AwJkW",
        "outputId": "933d42a9-3b09-4d9a-ed37-e0f9fb9bbae5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [04:32:12] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### XGBoost\n",
            "Acurácia: 0.9707602339181286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96        63\n",
            "           1       0.98      0.97      0.98       108\n",
            "\n",
            "    accuracy                           0.97       171\n",
            "   macro avg       0.97      0.97      0.97       171\n",
            "weighted avg       0.97      0.97      0.97       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LightGBM (Light Gradient Boosting Machine)**\n",
        "**Funcionamento**\n",
        "O LightGBM é uma implementação eficiente de Gradient Boosting desenvolvida pela Microsoft. Ele utiliza técnicas como histogram-based learning e leaf-wise growth para acelerar o treinamento e reduzir o consumo de memória, mantendo ou melhorando a precisão.\n",
        "\n",
        "**Pontos Fortes**\n",
        "- Rapidez: Treinamento mais rápido comparado ao XGBoost devido a otimizações internas.\n",
        "- Baixo Consumo de Memória: Eficiente no uso de memória, permitindo trabalhar com grandes conjuntos de dados.\n",
        "- Alto Desempenho: Similar ou superior ao XGBoost em muitas tarefas.\n",
        "- Suporte a Dados Categóricos: Lida de forma nativa com features categóricas.\n",
        "\n",
        "**Pontos Fracos**\n",
        "- Sensível aos Hiperparâmetros: Requer ajuste cuidadoso para alcançar o melhor desempenho.\n",
        "- Complexidade de Implementação: Pode ser mais complexo de implementar em algumas situações.\n",
        "- Menos Estável em Algumas - Configurações: Pode apresentar variações de desempenho dependendo dos dados e parâmetros."
      ],
      "metadata": {
        "id": "9DmXhh4KwJtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. LightGBM\n",
        "lgb_clf = lgb.LGBMClassifier(random_state=42)\n",
        "train_evaluate_model(lgb_clf, \"LightGBM\", X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfDqK_G4wWwh",
        "outputId": "89d341e0-e134-4ca1-fbf7-a433f2a5bf69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 249, number of negative: 149\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3978\n",
            "[LightGBM] [Info] Number of data points in the train set: 398, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.625628 -> initscore=0.513507\n",
            "[LightGBM] [Info] Start training from score 0.513507\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "### LightGBM\n",
            "Acurácia: 0.9473684210526315\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93        63\n",
            "           1       0.96      0.95      0.96       108\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.94      0.95      0.94       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n"
          ]
        }
      ]
    }
  ]
}